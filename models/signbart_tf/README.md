# SignBART TensorFlow - Arabic Sign Language Recognition

TensorFlow/Keras implementation of SignBART for Arabic sign language gesture recognition with full quantization support (PTQ and QAT).

## ğŸ¯ Features

- **Functional API Model**: QAT-ready architecture using Keras Functional API
- **LOSO Cross-Validation**: Leave-One-Signer-Out evaluation across 3 users
- **Full Dataset Training**: Train on all 12 users combined
- **Quantization Support**: 
  - Post-Training Quantization (PTQ)
  - Quantization-Aware Training (QAT) with optimized hyperparameters
  - Dynamic-range INT8 quantization (weights INT8, activations FP32)
- **TFLite Export**: Optimized models for mobile/edge deployment
- **Comprehensive Evaluation**: Accuracy metrics, confusion matrices, FLOPs calculation

---

## ğŸ“ Project Structure

```
signbart_tf/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ arabic-asl-90kpts.yaml       # Model configuration (90 keypoints: body + hands + face)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ arabic-asl-90kpts/           # Full dataset (all users)
â”‚   â”‚   â”œâ”€â”€ all/                     # All samples for full training
â”‚   â”‚   â”‚   â”œâ”€â”€ G01/ ... G10/
â”‚   â”‚   â”œâ”€â”€ label2id.json
â”‚   â”‚   â””â”€â”€ id2label.json
â”‚   â”œâ”€â”€ arabic-asl-90kpts_LOSO_user01/  # LOSO split for user01
â”‚   â”‚   â”œâ”€â”€ train/                   # Training samples (users 08, 11)
â”‚   â”‚   â”œâ”€â”€ test/                    # Test samples (user01)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoints_*/                   # Training checkpoints
â”œâ”€â”€ exports/                         # Quantized models
â”‚   â”œâ”€â”€ ptq_loso/                    # PTQ models (per user)
â”‚   â”œâ”€â”€ qat_loso/                    # QAT models (per user)
â”‚   â”œâ”€â”€ ptq_full/                    # PTQ model (full dataset)
â”‚   â””â”€â”€ qat_full/                    # QAT model (full dataset)
â””â”€â”€ results/                         # Evaluation results
    â”œâ”€â”€ confusion_matrices/          # Confusion matrix PNGs
    â”œâ”€â”€ model_info.csv               # Parameters & FLOPs
    â”œâ”€â”€ summary_table.csv            # Accuracy comparison
    â””â”€â”€ per_class_accuracy.csv       # Per-gesture accuracy
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
conda create -n signbart_tf python=3.10
conda activate signbart_tf
pip install tensorflow tensorflow-model-optimization keras pyyaml numpy matplotlib seaborn
```

### 2. Training Workflows

#### **LOSO Training (Recommended for Research)**

Train on 3 LOSO splits (leave-one-signer-out):

```bash
python train_loso_functional.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --base_data_path data/arabic-asl-90kpts \
    --epochs 80 \
    --lr 2e-4 \
    --seed 379
```

**Output**: 3 FP32 models in `checkpoints_arabic_asl_LOSO_user01/`, `user08/`, `user11/`

---

#### **Full Dataset Training**

Train on all 12 users:

```bash
python train_full_dataset.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --base_data_path data/arabic-asl-90kpts \
    --epochs 80 \
    --lr 2e-4 \
    --seed 42
```

**Output**: `checkpoints_arabic_asl_full/final_model.h5` and `final_model_fp32.tflite`

---

### 3. Quantization

#### **Post-Training Quantization (PTQ)**

Dynamic-range INT8 quantization (weights only):

```bash
# For LOSO models
python ptq_export_batch.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --base_data_path data/arabic-asl-90kpts

# For full dataset model
python ptq_export.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --checkpoint checkpoints_arabic_asl_full/final_model.h5 \
    --output_dir exports/ptq_full
```

---

#### **Quantization-Aware Training (QAT)**

Fine-tune with simulated quantization (better accuracy than PTQ):

```bash
# For LOSO models
python train_loso_functional_qat_batch.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --base_data_path data/arabic-asl-90kpts \
    --batch_size 4 \
    --qat_epochs 10 \
    --lr 5e-5

# For full dataset model
python train_loso_functional_qat.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --data_path data/arabic-asl-90kpts \
    --checkpoint checkpoints_arabic_asl_full/final_model.h5 \
    --output_dir exports/qat_full \
    --batch_size 4 \
    --qat_epochs 10 \
    --lr 5e-5 \
    --no_validation
```

**QAT Configuration**:
- **Learning Rate**: 5e-5 (~4Ã— lower than FP32 training)
- **Batch Size**: 4 (larger than training for stability)
- **Epochs**: 10-20 (short fine-tuning)
- **Quantized Layers**: All Dense layers (FFN, attention projections, projection layers)
- **Excluded**: Projection container (tuple output handling issue)
- **Gradient Clipping**: clipnorm=1.0
- **Early Stopping**: Patience 10 (restores best weights)

---

### 4. Evaluation

#### **Single Model Evaluation**

```bash
python evaluate_tflite_single.py \
    --config_path configs/arabic-asl-90kpts.yaml \
    --data_path data/arabic-asl-90kpts_LOSO_user01 \
    --split test \
    --tflite_path checkpoints_arabic_asl_full/final_model_fp32.tflite
```

---

#### **Comprehensive Results Collection**

Generate full report with confusion matrices, FLOPs, and accuracy tables:

```bash
python collect_results.py --run_evaluation
```

**Output**:
- `results/report_YYYYMMDD_HHMMSS.txt` - Full text report
- `results/confusion_matrices/*.png` - 9 confusion matrices (3 users Ã— 3 models)
- `results/model_info.csv` - Parameters, FLOPs
- `results/summary_table.csv` - FP32 vs PTQ vs QAT comparison
- `results/per_class_accuracy.csv` - Per-gesture accuracy

---

## ğŸ“Š Model Architecture

```
Input: Keypoints [T, 90, 2]
  â†“
Projection Layer (proj_x1, proj_y1) â†’ [T, d_model=144]
  â†“
Positional Embeddings (learned)
  â†“
Encoder (2 layers, 4 heads, FFN 576)
  â”œâ”€ Self-Attention (q_proj, k_proj, v_proj, out_proj)
  â”œâ”€ LayerNorm + Residual
  â”œâ”€ Feed-Forward (fc1, fc2)
  â””â”€ LayerNorm + Residual
  â†“
Decoder (2 layers, 4 heads, FFN 576)
  â”œâ”€ Causal Self-Attention
  â”œâ”€ Cross-Attention to Encoder
  â”œâ”€ Feed-Forward (fc1, fc2)
  â””â”€ LayerNorm + Residual
  â†“
Extract Last Valid Token
  â†“
Classification Head â†’ [10 classes]
```

**Parameters**: 773,578 total  
**Model Size**: 2.95 MB (FP32), ~0.75 MB (INT8)  
**FLOPs**: Calculated per forward pass  

---

## ğŸ”¬ Quantization Details

### What Gets Quantized

âœ… **Quantized** (Weights + Activations during training, Weights-only in TFLite):
- FFN Dense layers: `fc1`, `fc2` (in encoder & decoder)
- Attention projections: `q_proj`, `k_proj`, `v_proj`, `out_proj`
- Input projections: `proj_x1`, `proj_y1`
- Classification head: `out_proj`

âŒ **Not Quantized**:
- Embeddings (positional)
- Normalization layers (LayerNorm)
- Activation functions (GELU, Softmax)
- Dropout
- Structural operations (residual connections, masking)

ğŸš« **Excluded from Wrapping** (Critical):
- `Projection` container (causes collapse if wrapped, but internal Dense layers ARE quantized)

### Why Dynamic-Range Quantization?

We use **weights-only INT8 quantization** (dynamic-range) instead of full INT8 because:
- âœ… Significant model size reduction (~75% smaller)
- âœ… Numerically stable (avoids INF/NaN in attention & normalization)
- âœ… No calibration dataset needed
- âŒ Full INT8 (with calibration) caused numerical instability â†’ INF values

---

## ğŸ“ Key Findings (QAT Optimization)

### Training Stability Issues Solved

**Problem**: Model collapse after 3-4 QAT epochs (accuracy dropped from 95% â†’ 11%)

**Root Cause**: The `Projection` container layer (tuple output) was sensitive to `QuantizeWrapper`, even with `NoOpQuantizeConfig`.

**Solution**: 
1. Exclude `Projection` container from wrapping entirely
2. Still quantize its internal Dense layers (`proj_x1`, `proj_y1`) via filters
3. Use lower LR (5e-5 vs 2e-4 for FP32 training)
4. Increase batch size (4 vs 1 for FP32 training)
5. Add gradient clipping (clipnorm=1.0)
6. Early stopping with best-weight restoration

**Result**: Stable QAT training reaching 95% accuracy âœ…

### Attention Layers Are Safe to Quantize

**Myth**: Attention projections are too sensitive for quantization  
**Reality**: `q_proj`, `k_proj`, `v_proj`, `out_proj` can be safely quantized with proper hyperparameters

---

## ğŸ“ˆ Expected Results

### LOSO Cross-Validation (3 users)

| Model Type | Accuracy | Top-5 Acc | Size (MB) | Speedup |
|------------|----------|-----------|-----------|---------|
| FP32       | 94-96%   | 99-100%   | 3.00      | 1.0Ã—    |
| INT8-PTQ   | 93-95%   | 99-100%   | 0.75      | 2-3Ã—    |
| INT8-QAT   | 94-96%   | 99-100%   | 0.75      | 2-3Ã—    |

**QAT advantage**: +1-2% accuracy over PTQ while maintaining same size/speed.

---

## ğŸ› ï¸ Key Scripts Reference

### Training
- `train_loso_functional.py` - LOSO training (3 users)
- `train_full_dataset.py` - Full dataset training (12 users)
- `main_functional.py` - Core training logic (called by above)

### Quantization
- `ptq_export.py` - PTQ for single model
- `ptq_export_batch.py` - PTQ for all LOSO models
- `train_loso_functional_qat.py` - QAT for single model
- `train_loso_functional_qat_batch.py` - QAT for all LOSO models

### Evaluation
- `evaluate_tflite_single.py` - Evaluate any TFLite model on any dataset
- `collect_results.py` - Comprehensive report generation
- `test_tflite_models.py` - Compare FP32/PTQ/QAT side-by-side

### Utilities
- `dataset.py` - Dataset loading & preprocessing
- `model_functional.py` - Functional API model definition
- `layers.py` - Custom layers (Projection, ClassificationHead, etc.)
- `encoder.py`, `decoder.py`, `attention.py` - Architecture components

---

## ğŸ› Troubleshooting

### Issue: "FileNotFoundError: train split not found"

**Cause**: Using LOSO script on full dataset (or vice versa)

**Solution**:
- LOSO: Use `train_loso_functional_qat.py` with `data/arabic-asl-90kpts_LOSO_userXX`
- Full: Use `train_loso_functional_qat.py` with `data/arabic-asl-90kpts` (auto-detects `all` split)

---

### Issue: "Top5Accuracy deserialization error"

**Cause**: Mismatch between saved model config and metric definition

**Solution**: Already fixed in latest code (extracts `k` from kwargs)

---

### Issue: QAT model collapse

**Cause**: One of:
1. Wrapping `Projection` container
2. Learning rate too high
3. Batch size too small

**Solution**: Use provided QAT hyperparameters (lr=5e-5, batch=4)

---

## ğŸ“š Citation

```bibtex
@article{signbart2024,
  title={SignBART: Arabic Sign Language Recognition with Quantization},
  author={Your Name},
  year={2024}
}
```

---

## ğŸ“ License

[Your License Here]

---

## ğŸ¤ Contributing

Contributions welcome! Please ensure:
- Code follows TensorFlow/Keras best practices
- Quantization changes are tested on LOSO splits
- Documentation is updated

---

## ğŸ“§ Contact

[Your Contact Information]

